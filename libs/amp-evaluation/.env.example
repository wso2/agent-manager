# ============================================================================
# AMP-Eval Configuration Template
# ============================================================================
# INSTRUCTIONS:
# 1. Copy this file to .env: cp .env.example .env
# 2. Fill in your actual values in the .env file
# 3. The .env file is gitignored and will not be committed
# ============================================================================

# ============================================================================
# AGENT CONFIGURATION (Required)
# ============================================================================
# These identify the agent being evaluated

# Agent name (human-readable)
AGENT_NAME=my-customer-service-agent

# Agent unique identifier
AGENT_UID=agent_abc123def456

# Project this agent belongs to
PROJECT_UID=project_xyz789

# Environment where agent runs (dev, staging, prod)
ENVIRONMENT_UID=env_prod_001


# ============================================================================
# PLATFORM CONFIGURATION (Required for cloud mode)
# ============================================================================
# Agent Manager Platform settings

# Platform API endpoint
PLATFORM_API_URL=https://platform.example.com/api

# Platform API key for authentication
PLATFORM_API_KEY=your_platform_api_key_here

# Traceloop/OpenTelemetry endpoint
TRACELOOP_API_ENDPOINT=http://localhost:22893/otel

# Traceloop API key
TRACELOOP_KEY=your_traceloop_key_here


# ============================================================================
# TRACE LOADING (Production monitoring mode)
# ============================================================================
# Configuration for loading traces from production

# How to load traces: "platform" | "local" | "file"
TRACE_LOADER_MODE=platform

# For platform mode: trace storage configuration
TRACE_STORAGE_TYPE=opensearch

# OpenSearch/Elasticsearch settings
OPENSEARCH_URL=https://opensearch.example.com:9200
OPENSEARCH_USERNAME=admin
OPENSEARCH_PASSWORD=your_opensearch_password
OPENSEARCH_INDEX_PREFIX=traces-

# For file mode: local trace file path
TRACE_FILE_PATH=./traces/production_traces.json


# ============================================================================
# CHECKPOINT MANAGEMENT (Production monitoring mode)
# ============================================================================
# Track which traces have been evaluated to avoid re-processing

# Where to store checkpoint: "platform" | "local" | "redis"
CHECKPOINT_STORAGE=platform

# For local checkpoint storage
CHECKPOINT_FILE_PATH=./checkpoints/last_evaluated.json

# For Redis checkpoint storage
REDIS_URL=redis://localhost:6379/0
REDIS_KEY_PREFIX=amp-eval:checkpoint:


# ============================================================================
# EVALUATION CONFIGURATION
# ============================================================================
# Settings for running evaluations

# Evaluation mode: "benchmark" (with dataset) | "live" (continuous traces)
# - benchmark: Runs evaluations against a defined dataset with tasks and trials
# - live: Runs evaluations on continuous incoming trace data
EVAL_MODE=live

# Dataset configuration (REQUIRED when EVAL_MODE=benchmark)
# Explicit flag to indicate dataset is required
EVAL_HAS_DATASET=false

# Path to dataset file (required when EVAL_HAS_DATASET=true)
EVAL_DATASET_PATH=./datasets/my_benchmark.csv

# Dataset format: "csv" | "json"
EVAL_DATASET_FORMAT=csv

# Number of trials per task (for benchmark mode, to handle non-determinism)
EVAL_NUM_TRIALS_PER_TASK=1

# Batch size for processing traces
EVAL_BATCH_SIZE=100

# Maximum concurrent evaluations
EVAL_MAX_CONCURRENCY=10

# Evaluation timeout (seconds)
EVAL_TIMEOUT_SECONDS=30

# Number of retries for failed evaluations
EVAL_MAX_RETRIES=3


# ============================================================================
# SCHEDULER CONFIGURATION (Cloud/Cron mode)
# ============================================================================
# For scheduled evaluation runs

# Scheduler mode: "manual" | "cron" | "kubernetes"
SCHEDULER_MODE=cron

# Cron expression (e.g., "0 */6 * * *" = every 6 hours)
SCHEDULER_CRON_EXPRESSION=0 */6 * * *

# Trace lookback window (how far back to fetch traces)
# Format: ISO 8601 duration (e.g., PT24H = 24 hours, P7D = 7 days)
TRACE_LOOKBACK_WINDOW=PT24H

# Kubernetes CronJob settings (if using Kubernetes)
K8S_NAMESPACE=amp-eval
K8S_SERVICE_ACCOUNT=amp-eval-runner


# ============================================================================
# RESULTS PUBLISHING
# ============================================================================
# Where to send evaluation results

# Publish results to platform?
PUBLISH_RESULTS=true

# Platform results endpoint
RESULTS_API_URL=https://platform.example.com/api/evaluation-results

# Also save results locally?
SAVE_RESULTS_LOCALLY=true
RESULTS_LOCAL_PATH=./results/


# ============================================================================
# LLM CONFIGURATION (For LLM-as-judge evaluators)
# ============================================================================
# API keys for LLM-based evaluators

# OpenAI API key
OPENAI_API_KEY=sk-your-openai-key-here

# OpenAI model for evaluations
OPENAI_EVAL_MODEL=gpt-4

# Anthropic API key
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here

# Anthropic model for evaluations
ANTHROPIC_EVAL_MODEL=claude-3-sonnet-20240229

# Azure OpenAI settings (if using Azure)
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_KEY=your_azure_openai_key
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4


# ============================================================================
# EXTERNAL SERVICES (For specific evaluators)
# ============================================================================

# Tavily API key (for research agents)
TAVILY_API_KEY=tvly-your-tavily-key-here

# Perspective API key (for toxicity detection)
PERSPECTIVE_API_KEY=your_perspective_api_key

# Custom embedding service (for semantic similarity)
EMBEDDING_API_URL=https://embeddings.example.com/api
EMBEDDING_API_KEY=your_embedding_api_key


# ============================================================================
# DATASET CONFIGURATION
# ============================================================================
# Settings for loading evaluation datasets

# Default dataset location
DEFAULT_DATASET_PATH=./datasets/

# Dataset format: "csv" | "json" | "parquet"
DEFAULT_DATASET_FORMAT=csv

# Auto-reload datasets on change?
DATASET_AUTO_RELOAD=false


# ============================================================================
# LOGGING & MONITORING
# ============================================================================
# Logging configuration

# Log level: DEBUG | INFO | WARNING | ERROR | CRITICAL
LOG_LEVEL=INFO

# Log format: "json" | "text"
LOG_FORMAT=json

# Log output: "console" | "file" | "both"
LOG_OUTPUT=both

# Log file path (if LOG_OUTPUT includes "file")
LOG_FILE_PATH=./logs/amp-eval.log

# Enable performance metrics collection?
ENABLE_METRICS=true

# Metrics export endpoint (Prometheus, etc.)
METRICS_EXPORT_URL=http://localhost:9090/metrics


# ============================================================================
# PERFORMANCE TUNING
# ============================================================================

# Enable result caching?
ENABLE_RESULT_CACHE=true

# Cache TTL (seconds)
CACHE_TTL_SECONDS=3600

# Cache backend: "memory" | "redis" | "disk"
CACHE_BACKEND=memory

# Maximum memory for cache (MB)
CACHE_MAX_MEMORY_MB=512


# ============================================================================
# SECURITY
# ============================================================================

# Verify SSL certificates?
VERIFY_SSL=true

# Request timeout (seconds)
REQUEST_TIMEOUT=30

# API rate limiting (requests per minute)
RATE_LIMIT_PER_MINUTE=100


# ============================================================================
# DEVELOPMENT / DEBUG
# ============================================================================

# Enable debug mode?
DEBUG=false

# Enable verbose trace logging?
VERBOSE_TRACES=false

# Dry run mode (don't actually publish results)?
DRY_RUN=false

# Save intermediate evaluation data?
SAVE_INTERMEDIATE_DATA=false
INTERMEDIATE_DATA_PATH=./debug/
